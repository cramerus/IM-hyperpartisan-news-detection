{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy\n",
    "from glob import glob\n",
    "import sklearn as sk\n",
    "import ast\n",
    "import csv\n",
    "import re\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "csv.field_size_limit(100000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the functions used to extract the linguistic features from the spaCy tags previously created.\n",
    "\n",
    "There are also functions used to extract counts of words from specific lexica that have been referenced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# POS tag counts\n",
    "\n",
    "def count_adj(pos):\n",
    "    # takes unevaluated list of POS tags for a single text\n",
    "    pos = ast.literal_eval(pos)\n",
    "    if len(pos) == 0:\n",
    "        return 0.0\n",
    "    return pos.count('ADJ')/len(pos)\n",
    "\n",
    "def count_adv(pos):\n",
    "    # takes unevaluated list of POS tags for a single text\n",
    "    pos = ast.literal_eval(pos)\n",
    "    if len(pos) == 0:\n",
    "        return 0.0\n",
    "    return pos.count('ADV')/len(pos)\n",
    "\n",
    "def count_pron(pos):\n",
    "    # takes unevaluated list of POS tags for a single text\n",
    "    pos = ast.literal_eval(pos)\n",
    "    if len(pos) == 0:\n",
    "        return 0.0\n",
    "    return pos.count('PRON')/len(pos)\n",
    "\n",
    "def count_noun(pos):\n",
    "    # takes unevaluated list of POS tags for a single text\n",
    "    pos = ast.literal_eval(pos)\n",
    "    if len(pos) == 0:\n",
    "        return 0.0\n",
    "    return pos.count('NOUN')/len(pos)\n",
    "\n",
    "def count_propn(pos):\n",
    "    # takes unevaluated list of POS tags for a single text\n",
    "    pos = ast.literal_eval(pos)\n",
    "    if len(pos) == 0:\n",
    "        return 0.0\n",
    "    return pos.count('PROPN')/len(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MPQA lexicons: intensifiers, subjectivity clues (strong, weak)\n",
    "\n",
    "with open('lexica/intensifiers.tff') as f:\n",
    "    intensifiers = [line[29:-15].split(' ')[0] for line in f.readlines()]\n",
    "    \n",
    "with open('lexica/subjclueslen1-HLTEMNLP05.tff') as f:\n",
    "    clues = [(line[5:].split(' ')[0], line[5:].split(' ')[2][6:]) for line in f.readlines()]\n",
    "    strongsubj = []\n",
    "    weaksubj = []\n",
    "    for strength, word in clues:\n",
    "        if strength == 'weaksubj':\n",
    "            weaksubj.append(word)\n",
    "        else:\n",
    "            strongsubj.append(word)\n",
    "            \n",
    "def count_strongsubj(lemmas):\n",
    "    lemmas = ast.literal_eval(lemmas)\n",
    "    if len(lemmas) == 0:\n",
    "        return 0.0\n",
    "    clues = 0\n",
    "    for word in strongsubj:\n",
    "        if word in lemmas:\n",
    "            clues += 1\n",
    "    return clues/len(set(lemmas))\n",
    "\n",
    "def count_weaksubj(lemmas):\n",
    "    lemmas = ast.literal_eval(lemmas)\n",
    "    if len(lemmas) == 0:\n",
    "        return 0.0\n",
    "    clues = 0\n",
    "    for word in weaksubj:\n",
    "        if word in lemmas:\n",
    "            clues += 1\n",
    "    return clues/len(set(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count passives, modality\n",
    "\n",
    "def count_aux(dep):\n",
    "    dep = ast.literal_eval(dep)\n",
    "    if len(dep) == 0:\n",
    "        return 0.0\n",
    "    return (dep.count('aux') + dep.count('auxpass')) / len(dep)\n",
    "\n",
    "def count_pass(dep):\n",
    "    dep = ast.literal_eval(dep)\n",
    "    if len(dep) == 0:\n",
    "        return 0.0\n",
    "    return (dep.count('nsubjpass') + dep.count('csubjpass')) / len(dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# readability features\n",
    "\n",
    "def avg_sent_length(text):\n",
    "    if type(text) != str:\n",
    "        return 0.0\n",
    "    words = nltk.word_tokenize(text)\n",
    "    sents = nltk.sent_tokenize(text)\n",
    "    if len(sents) == 0:\n",
    "        return 0.0\n",
    "    return len(words)/len(sents)\n",
    "\n",
    "def avg_word_length(text):\n",
    "    if type(text) != str:\n",
    "        return 0.0\n",
    "    words = nltk.word_tokenize(text)\n",
    "    if len(words) == 0:\n",
    "        return 0.0\n",
    "    return len(text)/len(words)\n",
    "\n",
    "def exclamation_marks(text):\n",
    "    if type(text) != str:\n",
    "        return 0.0\n",
    "    if len(text) == 0:\n",
    "        return 0.0\n",
    "    return text.count('!')/len(text)\n",
    "\n",
    "def question_marks(text):\n",
    "    if type(text) != str:\n",
    "        return 0.0\n",
    "    if len(text) == 0:\n",
    "        return 0.0\n",
    "    return text.count('?')/len(text)\n",
    "\n",
    "def multiple_punct(text):\n",
    "    if type(text) != str:\n",
    "        return 0.0\n",
    "    if len(text) == 0:\n",
    "        return 0.0\n",
    "    count = 0\n",
    "    count += text.count('!!')\n",
    "    count += text.count('??')\n",
    "    count += text.count('?!')\n",
    "    count += text.count('!?')\n",
    "    return count/len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EmoLex\n",
    "\n",
    "emotions = defaultdict(lambda: [])\n",
    "\n",
    "with open('lexica/NRC-Sentiment-Emotion-Lexicons/NRC-Affect-Intensity-Lexicon/NRC-AffectIntensity-Lexicon.txt') as f:\n",
    "    pairs = [(line[:-1].split('\\t')[0], line[:-1].split('\\t')[2]) for line in f.readlines()[1:]]\n",
    "    for word, emotion in pairs:\n",
    "        emotions[emotion].append(word)\n",
    "\n",
    "def count_anger(lemmas):\n",
    "    lemmas = ast.literal_eval(lemmas)\n",
    "    if len(lemmas) == 0:\n",
    "        return 0.0\n",
    "    clues = 0\n",
    "    for word in emotions['anger']:\n",
    "        if word in lemmas:\n",
    "            clues += 1\n",
    "    return clues/len(set(lemmas))\n",
    "\n",
    "def count_fear(lemmas):\n",
    "    lemmas = ast.literal_eval(lemmas)\n",
    "    if len(lemmas) == 0:\n",
    "        return 0.0\n",
    "    clues = 0\n",
    "    for word in emotions['fear']:\n",
    "        if word in lemmas:\n",
    "            clues += 1\n",
    "    return clues/len(set(lemmas))\n",
    "\n",
    "def count_sadness(lemmas):\n",
    "    lemmas = ast.literal_eval(lemmas)\n",
    "    if len(lemmas) == 0:\n",
    "        return 0.0\n",
    "    clues = 0\n",
    "    for word in emotions['sadness']:\n",
    "        if word in lemmas:\n",
    "            clues += 1\n",
    "    return clues/len(set(lemmas))\n",
    "\n",
    "def count_joy(lemmas):\n",
    "    lemmas = ast.literal_eval(lemmas)\n",
    "    if len(lemmas) == 0:\n",
    "        return 0.0\n",
    "    clues = 0\n",
    "    for word in emotions['joy']:\n",
    "        if word in lemmas:\n",
    "            clues += 1\n",
    "    return clues/len(set(lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hapax Legomena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def countvocab(df):\n",
    "    \n",
    "    vocab = Counter()\n",
    "    word = re.compile('[a-z]*\\Z')\n",
    "    \n",
    "    #except_file = open('token_exceptions.txt')\n",
    "    #exceptions = except_file.read().split('\\n')\n",
    "    #except_file.close()\n",
    "    #stopwords_file = open('sklearn_stopwords.txt')\n",
    "    #stopwords = stopwords_file.read().split('\\n')\n",
    "    #stopwords_file.close()\n",
    "    \n",
    "    for text in df['text_lemmas']:\n",
    "        for token in ast.literal_eval(text):\n",
    "            if word.match(token) == None:\n",
    "                continue\n",
    "            #if token in exceptions:\n",
    "            #    continue\n",
    "            #if token in stopwords:\n",
    "            #    continue\n",
    "            vocab[token] += 1\n",
    "            \n",
    "    hapax_legomena = [word for word in vocab if vocab[word]==1]\n",
    "            \n",
    "    true = 0\n",
    "    true_total = 0\n",
    "    false = 0\n",
    "    false_total = 0\n",
    "    \n",
    "    for row, item in df.iterrows():\n",
    "        if item['label'] == True:\n",
    "            true_total += 1\n",
    "        else:\n",
    "            false_total += 1\n",
    "            \n",
    "        if len(list(set(hapax_legomena).intersection(ast.literal_eval(item['text_lemmas'])))) != 0:\n",
    "            if item['label'] == True:\n",
    "                true += 1\n",
    "            else:\n",
    "                false += 1\n",
    "    \n",
    "    print('Proportion of biased articles with hapax legomena: ' + str(float(true) / true_total))\n",
    "    print('Proportion of unbiased articles with hapax legomena: ' + str(float(false) / false_total))\n",
    "    \n",
    "    return hapax_legomena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of biased articles with hapax legomena: 0.4539581043727312\n",
      "Proportion of unbiased articles with hapax legomena: 0.3920528475628065\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['canadean',\n",
       " 'lbix',\n",
       " 'happywater',\n",
       " 'lapic',\n",
       " 'waterbox',\n",
       " 'undermotivated',\n",
       " 'condesa',\n",
       " 'mayekiso',\n",
       " 'tiwi',\n",
       " 'redesignate',\n",
       " 'tudeh',\n",
       " 'nonpeak',\n",
       " 'prologistix',\n",
       " 'dibella',\n",
       " 'kassicieh',\n",
       " 'aisles',\n",
       " 'locater',\n",
       " 'overprocess',\n",
       " 'hilar',\n",
       " 'kittens',\n",
       " 'dcfc',\n",
       " 'blung',\n",
       " 'dyrice',\n",
       " 'vanishedwill',\n",
       " 'minkowing',\n",
       " 'hoedown',\n",
       " 'manian',\n",
       " 'lanni',\n",
       " 'gooden',\n",
       " 'gorse',\n",
       " 'senara',\n",
       " 'poldark',\n",
       " 'smit',\n",
       " 'shoeprint',\n",
       " 'porthcurno',\n",
       " 'harborfront',\n",
       " 'godrevy',\n",
       " 'bodelva',\n",
       " 'pentewan',\n",
       " 'overgrowth',\n",
       " 'marazion',\n",
       " 'bossiney',\n",
       " 'barnoon',\n",
       " 'riseat',\n",
       " 'contenton',\n",
       " 'tocover',\n",
       " 'assetsof',\n",
       " 'leadthe',\n",
       " 'onfacebook',\n",
       " 'gamoloco',\n",
       " 'obstat',\n",
       " 'montreat',\n",
       " 'churchworks',\n",
       " 'youthministry',\n",
       " 'chandon',\n",
       " 'sweatily',\n",
       " 'bumfordshire',\n",
       " 'visitingdisney',\n",
       " 'parentcomcast',\n",
       " 'activization',\n",
       " 'reasonability',\n",
       " 'nivida',\n",
       " 'sinawhen',\n",
       " 'ethelr',\n",
       " 'svein',\n",
       " 'forkbeard',\n",
       " 'dansk',\n",
       " 'overbr',\n",
       " 'inutterably',\n",
       " 'soberminded',\n",
       " 'sunnylands',\n",
       " 'harping',\n",
       " 'destructur',\n",
       " 'pagnoulle',\n",
       " 'significantincrease',\n",
       " 'medicineoliceridine',\n",
       " 'isoliceridine',\n",
       " 'harmanli',\n",
       " 'bazi',\n",
       " 'sodomized',\n",
       " 'bereesh',\n",
       " 'pieder',\n",
       " 'tribualtion',\n",
       " 'standest',\n",
       " 'highmind',\n",
       " 'privledge',\n",
       " 'heavenlies',\n",
       " 'warrish',\n",
       " 'philbyesque',\n",
       " 'myspie',\n",
       " 'ormutiny',\n",
       " 'teethed',\n",
       " 'weemsish',\n",
       " 'culpers',\n",
       " 'summoner',\n",
       " 'simcoe',\n",
       " 'blofeldian',\n",
       " 'spycrafty',\n",
       " 'megamillionaire',\n",
       " 'shalett',\n",
       " 'abiteboul',\n",
       " 'wehrlein',\n",
       " 'elfendahl',\n",
       " 'vaarsi',\n",
       " 'philado',\n",
       " 'chocking',\n",
       " 'oesterle',\n",
       " 'monfrini',\n",
       " 'slaphillary',\n",
       " 'thp',\n",
       " 'uttecht',\n",
       " 'zebras',\n",
       " 'budaj',\n",
       " 'dekeyser',\n",
       " 'enni',\n",
       " 'ousama',\n",
       " 'amazonair',\n",
       " 'mopeds',\n",
       " 'camerasa',\n",
       " 'spetrum',\n",
       " 'hipchat',\n",
       " 'koenigsbauer',\n",
       " 'russkaya',\n",
       " 'vesna',\n",
       " 'khorosheva',\n",
       " 'markovtsy',\n",
       " 'cenna',\n",
       " 'pavehawk',\n",
       " 'factored',\n",
       " 'weinswig',\n",
       " 'estimatedthat',\n",
       " 'claima',\n",
       " 'nacoula',\n",
       " 'joud',\n",
       " 'shehaby',\n",
       " 'kneipe',\n",
       " 'pilsener',\n",
       " 'pilseners',\n",
       " 'radler',\n",
       " 'clausthaler',\n",
       " 'holsten',\n",
       " 'warsteiner',\n",
       " 'birte',\n",
       " 'kleppien',\n",
       " 'extendedanother',\n",
       " 'interenet',\n",
       " 'bandjak',\n",
       " 'pivo',\n",
       " 'chetnik',\n",
       " 'topole',\n",
       " 'ravne',\n",
       " 'ranchera',\n",
       " 'sgp',\n",
       " 'mingled',\n",
       " 'godparent',\n",
       " 'immoderation',\n",
       " 'linnet',\n",
       " 'oystercatcher',\n",
       " 'choonhaven',\n",
       " 'suphat',\n",
       " 'hasuwanakit',\n",
       " 'kraprayoon',\n",
       " 'chulachomklao',\n",
       " 'phut',\n",
       " 'kutajitto',\n",
       " 'jumsai',\n",
       " 'sulak',\n",
       " 'sivaraksa',\n",
       " 'bangkokians',\n",
       " 'siamwalla',\n",
       " 'weng',\n",
       " 'tojirakara',\n",
       " 'chakri',\n",
       " 'tantivejkul',\n",
       " 'tinsulanonda',\n",
       " 'rahong',\n",
       " 'zoido',\n",
       " 'evron',\n",
       " 'fbb',\n",
       " 'verrelli',\n",
       " 'medullablastoma',\n",
       " 'exercisable',\n",
       " 'aheadand',\n",
       " 'paamco',\n",
       " 'anora',\n",
       " 'dubyah',\n",
       " 'repluck',\n",
       " 'mutha',\n",
       " 'schleschinger',\n",
       " 'bulow',\n",
       " 'nethrerlands',\n",
       " 'kidnaping',\n",
       " 'exceptance',\n",
       " 'arbys',\n",
       " 'particpate',\n",
       " 'kathrynn',\n",
       " 'unwrappig',\n",
       " 'matott',\n",
       " 'quenese',\n",
       " 'legerski',\n",
       " 'chisti',\n",
       " 'neighborh',\n",
       " 'thatha',\n",
       " 'shabtai',\n",
       " 'debkafile',\n",
       " 'securocrat',\n",
       " 'sharonist',\n",
       " 'livein',\n",
       " 'hillbruner',\n",
       " 'doulier',\n",
       " 'ghanxi',\n",
       " 'zubeyr',\n",
       " 'guitarmy',\n",
       " 'spokescouncil',\n",
       " 'cognescenti',\n",
       " 'thiscantbehappenng',\n",
       " 'poltras',\n",
       " 'tranportation',\n",
       " 'sapui',\n",
       " 'rathmell',\n",
       " 'bektas',\n",
       " 'khmelevka',\n",
       " 'steregushchy',\n",
       " 'nastoichivy',\n",
       " 'gorshkov',\n",
       " 'riekstins',\n",
       " 'gsouth',\n",
       " 'tralfamadorian',\n",
       " 'rosenberger',\n",
       " 'yoaz',\n",
       " 'amaury',\n",
       " 'golbricht',\n",
       " 'devoti',\n",
       " 'olatunji',\n",
       " 'ishaya',\n",
       " 'yahaya',\n",
       " 'rotimi',\n",
       " 'hikima',\n",
       " 'onuah',\n",
       " 'camilus',\n",
       " 'eboh',\n",
       " 'akwagyiram',\n",
       " 'wva',\n",
       " 'resonating',\n",
       " 'libbys',\n",
       " 'kaiho',\n",
       " 'oshima',\n",
       " 'impactor',\n",
       " 'gunatanamo',\n",
       " 'katulis',\n",
       " 'jesu',\n",
       " 'hilfi',\n",
       " 'diethat',\n",
       " 'diemary',\n",
       " 'cicinelli',\n",
       " 'piegate',\n",
       " 'bollick',\n",
       " 'niah',\n",
       " 'jinyang',\n",
       " 'tablets',\n",
       " 'akleh',\n",
       " 'steffanie',\n",
       " 'nonspeaking',\n",
       " 'vidsummit',\n",
       " 'unasthetically',\n",
       " 'profil',\n",
       " 'prevelance',\n",
       " 'whatchou',\n",
       " 'transpo',\n",
       " 'holla',\n",
       " 'balaram',\n",
       " 'mahalder',\n",
       " 'saleemul',\n",
       " 'harjeet',\n",
       " 'manjeet',\n",
       " 'onur',\n",
       " 'maggelet',\n",
       " 'travelcenters',\n",
       " 'travelcenter',\n",
       " 'hummon',\n",
       " 'bathymetry',\n",
       " 'anemometer',\n",
       " 'wavetrack',\n",
       " 'skagen',\n",
       " 'stormsurf',\n",
       " 'swellwatch',\n",
       " 'mcintrye',\n",
       " 'maragh',\n",
       " 'archarcharch',\n",
       " 'maurinho',\n",
       " 'grable',\n",
       " 'flavien',\n",
       " 'huevo',\n",
       " 'bocachica',\n",
       " 'mops',\n",
       " 'emisael',\n",
       " 'anabella',\n",
       " 'unstablenthemornin',\n",
       " 'highborn',\n",
       " 'fillies',\n",
       " 'bisono',\n",
       " 'vertrazzo',\n",
       " 'geisha',\n",
       " 'blanconia',\n",
       " 'crumlin',\n",
       " 'waun',\n",
       " 'gryder',\n",
       " 'snitzel',\n",
       " 'terravista',\n",
       " 'geet',\n",
       " 'supermare',\n",
       " 'folkswood',\n",
       " 'ranvet',\n",
       " 'toorak',\n",
       " 'vivlos',\n",
       " 'mirco',\n",
       " 'yushun',\n",
       " 'himba',\n",
       " 'piere',\n",
       " 'vodacom',\n",
       " 'cracksman',\n",
       " 'qipco',\n",
       " 'louisvillian',\n",
       " 'mrns',\n",
       " 'aminobutyric',\n",
       " 'teters',\n",
       " 'happeneth',\n",
       " 'ensnarement',\n",
       " 'ponerologically',\n",
       " 'graduality',\n",
       " 'diagrammatic',\n",
       " 'rationaliz',\n",
       " 'ponerological',\n",
       " 'metastasising',\n",
       " 'apperceive',\n",
       " 'ponerology',\n",
       " 'surfersfortrump',\n",
       " 'boycottmegynkelly',\n",
       " 'shamkhani',\n",
       " 'pronouncedkhamenei',\n",
       " 'narnian',\n",
       " 'guliani',\n",
       " 'frisked',\n",
       " 'pookie',\n",
       " 'mauviniere',\n",
       " 'crepin',\n",
       " 'joachin',\n",
       " 'darina',\n",
       " 'haskovo',\n",
       " 'veselin',\n",
       " 'toshkov',\n",
       " 'refet',\n",
       " 'indepedent',\n",
       " 'migrations',\n",
       " 'chiapa',\n",
       " 'salvadorean',\n",
       " 'cleon',\n",
       " 'fredrickson',\n",
       " 'lwretch',\n",
       " 'bkw',\n",
       " 'commentsmade',\n",
       " 'petroulakis',\n",
       " 'suenos',\n",
       " 'terhi',\n",
       " 'kinnunen',\n",
       " 'unaffordably',\n",
       " 'unitar',\n",
       " 'defensewaste',\n",
       " 'administrationdoctors',\n",
       " 'againstkbr',\n",
       " 'pollutionit',\n",
       " 'pdce',\n",
       " 'concheso',\n",
       " 'thatrussia',\n",
       " 'moringa',\n",
       " 'picanza',\n",
       " 'jouaneh',\n",
       " 'aguel',\n",
       " 'biron',\n",
       " 'mallar',\n",
       " 'tmk',\n",
       " 'angelhack',\n",
       " 'gopman',\n",
       " 'batmasians',\n",
       " 'hillebrand',\n",
       " 'delaerentis',\n",
       " 'sinnreich',\n",
       " 'nelio',\n",
       " 'gryner',\n",
       " 'mazzillo',\n",
       " 'outlandishness',\n",
       " 'journaiism',\n",
       " 'sagging',\n",
       " 'hunkish',\n",
       " 'esten',\n",
       " 'sountrack',\n",
       " 'chiuck',\n",
       " 'trevone',\n",
       " 'eforensics',\n",
       " 'pietrelcina',\n",
       " 'voluptuary',\n",
       " 'olic',\n",
       " 'jpii',\n",
       " 'bevilacqua',\n",
       " 'uncurious',\n",
       " 'uncloseting',\n",
       " 'homosocial',\n",
       " 'marovich',\n",
       " 'sitq',\n",
       " 'strangler',\n",
       " 'benidorm',\n",
       " 'walley',\n",
       " 'breann',\n",
       " 'lanique',\n",
       " 'janae',\n",
       " 'sorrels',\n",
       " 'christiansfebruary',\n",
       " 'pushfebruary',\n",
       " 'trumpmarch',\n",
       " 'arbez',\n",
       " 'densify',\n",
       " 'reurbanization',\n",
       " 'linser',\n",
       " 'worldto',\n",
       " 'truthto',\n",
       " 'discoveryto',\n",
       " 'perspectivesto',\n",
       " 'poetryto',\n",
       " 'wrongto',\n",
       " 'downprecisely',\n",
       " 'corporationsyou',\n",
       " 'peoplesthat',\n",
       " 'plunderyou',\n",
       " 'havesettl',\n",
       " 'cinatti',\n",
       " 'ganzuri',\n",
       " 'tappersullivanmaherparkerstelterfranken',\n",
       " 'dogwhistles',\n",
       " 'ramps',\n",
       " 'receivedeleven',\n",
       " 'bocaue',\n",
       " 'rewound',\n",
       " 'acg',\n",
       " 'mckines',\n",
       " 'hamidu',\n",
       " 'hernst',\n",
       " 'gillenwater',\n",
       " 'avdiyika',\n",
       " 'motuzyanyk',\n",
       " 'vdiyika',\n",
       " 'poltorak',\n",
       " 'tavrida',\n",
       " 'retailermight',\n",
       " 'inunder',\n",
       " 'athelp',\n",
       " 'guardianciar',\n",
       " 'byrnetuesday',\n",
       " 'nakuru',\n",
       " 'gleiss',\n",
       " 'thundershower',\n",
       " 'jensenius',\n",
       " 'wildflie',\n",
       " 'rairden',\n",
       " 'kenema',\n",
       " 'yozwiak',\n",
       " 'pardis',\n",
       " 'sabeti',\n",
       " 'addam',\n",
       " 'sarick',\n",
       " 'noster',\n",
       " 'rippetoe',\n",
       " 'morlan',\n",
       " 'felmy',\n",
       " 'southen',\n",
       " 'trippingly',\n",
       " 'haka',\n",
       " 'wordsworthno',\n",
       " 'conall',\n",
       " 'bacchant',\n",
       " 'antigones',\n",
       " 'zeami',\n",
       " 'temperonce',\n",
       " 'salmydessus',\n",
       " 'dirce',\n",
       " 'fugal',\n",
       " 'geburt',\n",
       " 'aus',\n",
       " 'geiste',\n",
       " 'theban',\n",
       " 'sophocle',\n",
       " 'anouilh',\n",
       " 'ifhe',\n",
       " 'bodhran',\n",
       " 'aulo',\n",
       " 'uillean',\n",
       " 'phaedriades',\n",
       " 'eleutheria',\n",
       " 'unblest',\n",
       " 'delicia',\n",
       " 'broadnax',\n",
       " 'poyen',\n",
       " 'subpoenae',\n",
       " 'wawas',\n",
       " 'paulestinian',\n",
       " 'sooper',\n",
       " 'arbitrational',\n",
       " 'huldah',\n",
       " 'lic',\n",
       " 'lamell',\n",
       " 'mashabledescrib',\n",
       " 'outerspace',\n",
       " 'extraterrestial',\n",
       " 'cannibalise',\n",
       " 'pedlar',\n",
       " 'timothe',\n",
       " 'nasime',\n",
       " 'equistar',\n",
       " 'ozer',\n",
       " 'eskandar',\n",
       " 'kpvi',\n",
       " 'econofact',\n",
       " 'melitz',\n",
       " 'slobbo',\n",
       " 'nelan',\n",
       " 'rockler',\n",
       " 'bnpqy',\n",
       " 'ppdi',\n",
       " 'allysia',\n",
       " 'onbiologic',\n",
       " 'bioepis',\n",
       " 'peppa',\n",
       " 'kwik',\n",
       " 'squishees',\n",
       " 'repackager',\n",
       " 'fees',\n",
       " 'luncheonette',\n",
       " 'conern',\n",
       " 'lonelyhearts',\n",
       " 'gennelman',\n",
       " 'qvestion',\n",
       " 'disgorging',\n",
       " 'littlest',\n",
       " 'lashelle',\n",
       " 'compson',\n",
       " 'projectionist',\n",
       " 'tazwell',\n",
       " 'ellet',\n",
       " 'wascom',\n",
       " 'hsia',\n",
       " 'includesvote',\n",
       " 'canardon',\n",
       " 'pressmitch',\n",
       " 'silvermont',\n",
       " 'goldmont',\n",
       " 'iturrieta',\n",
       " 'chaiten',\n",
       " 'corcovado',\n",
       " 'devalve',\n",
       " 'jabrill',\n",
       " 'colquitt',\n",
       " 'bhge',\n",
       " 'haniyah',\n",
       " 'cisnormativity',\n",
       " 'zie',\n",
       " 'zir',\n",
       " 'baenziger',\n",
       " 'withtime',\n",
       " 'citire',\n",
       " 'dumpee',\n",
       " 'spanning',\n",
       " 'moneylife',\n",
       " 'dsea',\n",
       " 'comunications',\n",
       " 'oogle',\n",
       " 'kgby',\n",
       " 'amfm',\n",
       " 'kste',\n",
       " 'yor',\n",
       " 'advertisementand',\n",
       " 'werthein',\n",
       " 'bazaarvoice',\n",
       " 'yahoos',\n",
       " 'theglobal',\n",
       " 'companyprovide',\n",
       " 'masturbating',\n",
       " 'iconization',\n",
       " 'ripoffs',\n",
       " 'requirementst',\n",
       " 'bofaml',\n",
       " 'whittenberg',\n",
       " 'chetien',\n",
       " 'develope',\n",
       " 'uvongo',\n",
       " 'thermageddon',\n",
       " 'shippingport',\n",
       " 'prashanti',\n",
       " 'nilayam',\n",
       " 'puttaparthi',\n",
       " 'timesnewspaper',\n",
       " 'mistweets',\n",
       " 'selcte',\n",
       " 'revoluntary',\n",
       " 'emler',\n",
       " 'lacygne',\n",
       " 'ratepayers',\n",
       " 'publisherformer',\n",
       " 'parsenn',\n",
       " 'contentblog',\n",
       " 'talibanism',\n",
       " 'mismarket',\n",
       " 'depakote',\n",
       " 'risperidone',\n",
       " 'medications',\n",
       " 'symbicort',\n",
       " 'accolate',\n",
       " 'orfeu',\n",
       " 'artifacts',\n",
       " 'monareta',\n",
       " 'picotero',\n",
       " 'champeta',\n",
       " 'alok',\n",
       " 'swarup',\n",
       " 'grammercy',\n",
       " 'gmpresident',\n",
       " 'labrahimi',\n",
       " 'planters',\n",
       " 'riede',\n",
       " 'wfpl',\n",
       " 'adreleas',\n",
       " 'hillclimber',\n",
       " 'stumb',\n",
       " 'rosaviation',\n",
       " 'lathis',\n",
       " 'gwalior',\n",
       " 'jujuan',\n",
       " 'iwundu',\n",
       " 'moneke',\n",
       " 'scooooooooochie',\n",
       " 'truckloads',\n",
       " 'calleguas',\n",
       " 'feelingswhen',\n",
       " 'sizedif',\n",
       " 'ballmost',\n",
       " 'dsv',\n",
       " 'prebuilt',\n",
       " 'uwcnm',\n",
       " 'ifsra',\n",
       " 'nama',\n",
       " 'peripheries',\n",
       " 'moneymaking',\n",
       " 'hafizi',\n",
       " 'strandard',\n",
       " 'portably',\n",
       " 'grouperchek',\n",
       " 'puremolecular',\n",
       " 'safina',\n",
       " 'patrico',\n",
       " 'ruiloba',\n",
       " 'mustansiriya',\n",
       " 'madhi',\n",
       " 'wintergreen',\n",
       " 'mexlavu',\n",
       " 'vencap',\n",
       " 'raburn',\n",
       " 'pva',\n",
       " 'dohner',\n",
       " 'statists',\n",
       " 'astravas',\n",
       " 'licencees',\n",
       " 'almeda',\n",
       " 'lookey',\n",
       " 'adesman',\n",
       " 'brandisher',\n",
       " 'recored',\n",
       " 'mangham',\n",
       " 'devaun',\n",
       " 'plastictakeout',\n",
       " 'musiclisten',\n",
       " 'bulkbefore',\n",
       " 'dietingif',\n",
       " 'costskeep',\n",
       " 'merchandisethere',\n",
       " 'listit',\n",
       " 'tacticsin',\n",
       " 'iraqui',\n",
       " 'excellance',\n",
       " 'tiap',\n",
       " 'fantasmatically',\n",
       " 'predicter',\n",
       " 'autoerotic',\n",
       " 'vergil',\n",
       " 'pynchonian',\n",
       " 'superstructural',\n",
       " 'traumatically',\n",
       " 'posibility',\n",
       " 'iraqor',\n",
       " 'grandosity',\n",
       " 'libidnal',\n",
       " 'nukie',\n",
       " 'tibbet',\n",
       " 'timethat',\n",
       " 'obsessionalism',\n",
       " 'hosannah',\n",
       " 'tangling',\n",
       " 'extramartial',\n",
       " 'incentivizing',\n",
       " 'barnhouse',\n",
       " 'ageorges',\n",
       " 'gores',\n",
       " 'industryit',\n",
       " 'sures',\n",
       " 'bajaria',\n",
       " 'marmont',\n",
       " 'esmail',\n",
       " 'wif',\n",
       " 'tituss',\n",
       " 'thesps',\n",
       " 'daymond',\n",
       " 'empgasiz',\n",
       " 'ihamuotila',\n",
       " 'pullola',\n",
       " 'handelbanken',\n",
       " 'craddocks',\n",
       " 'okafur',\n",
       " 'clearpool',\n",
       " 'teevan',\n",
       " 'chugach',\n",
       " 'naso',\n",
       " 'workroom',\n",
       " 'carrabba',\n",
       " 'mongtomery',\n",
       " 'modulus',\n",
       " 'melliah',\n",
       " 'lenfest',\n",
       " 'bpoa',\n",
       " 'lavaughn',\n",
       " 'obligations',\n",
       " 'reservaton',\n",
       " 'carvedilol',\n",
       " 'diltiazem',\n",
       " 'verapamil',\n",
       " 'sants',\n",
       " 'kheradpir',\n",
       " 'madavo',\n",
       " 'samarco',\n",
       " 'arppu',\n",
       " 'garlinda',\n",
       " 'aymond',\n",
       " 'sammons',\n",
       " 'landowners',\n",
       " 'asimetelstat',\n",
       " 'australiabbcchild',\n",
       " 'helisolutions',\n",
       " 'maf',\n",
       " 'mekere',\n",
       " 'naring',\n",
       " 'interlaken',\n",
       " 'bustamonte',\n",
       " 'petrossian',\n",
       " 'argent',\n",
       " 'cormack',\n",
       " 'croome',\n",
       " 'leichhardt',\n",
       " 'crossbench',\n",
       " 'foxtel',\n",
       " 'teoh',\n",
       " 'maner',\n",
       " 'devilment',\n",
       " 'parkdale',\n",
       " 'julina',\n",
       " 'alliteratively',\n",
       " 'esping',\n",
       " 'decommodify',\n",
       " 'polanyians',\n",
       " 'maisano',\n",
       " 'pollyana',\n",
       " 'tums',\n",
       " 'orbs',\n",
       " 'backpfeifengesicht',\n",
       " 'scriptnotes',\n",
       " 'conquerer',\n",
       " 'horschel',\n",
       " 'auub',\n",
       " 'potrykus',\n",
       " 'decarus',\n",
       " 'hots',\n",
       " 'distributionnow',\n",
       " 'countha',\n",
       " 'meigan',\n",
       " 'marzooq',\n",
       " 'seyadi',\n",
       " 'addictiveness',\n",
       " 'dependsupon',\n",
       " 'whitechicken',\n",
       " 'poemswould',\n",
       " 'younghappy',\n",
       " 'fatethan',\n",
       " 'wisdomlady',\n",
       " 'thanyour',\n",
       " 'figurespull',\n",
       " 'apartquestion',\n",
       " 'progressdo',\n",
       " 'enjambment',\n",
       " 'ttlevision',\n",
       " 'sloppily',\n",
       " 'automobileswhen',\n",
       " 'olumuyiwa',\n",
       " 'aliu',\n",
       " 'safezone',\n",
       " 'tehune',\n",
       " 'maharat',\n",
       " 'kanine',\n",
       " 'recordsgrizzly',\n",
       " 'brooklandia',\n",
       " 'droste',\n",
       " 'fingerpicked',\n",
       " 'korine',\n",
       " 'kreayshawn',\n",
       " 'haterade',\n",
       " 'abqwomen',\n",
       " 'oostmann',\n",
       " 'sackett',\n",
       " 'yanun',\n",
       " 'khalizad',\n",
       " 'fashioning',\n",
       " 'moubadara',\n",
       " 'rawia',\n",
       " 'shawa',\n",
       " 'joerey',\n",
       " 'membreno',\n",
       " 'supporterswith',\n",
       " 'peleton',\n",
       " 'tomiwa',\n",
       " 'kring',\n",
       " 'goble',\n",
       " 'slewett',\n",
       " 'lasher',\n",
       " 'emptage',\n",
       " 'caserta',\n",
       " 'gathegi',\n",
       " 'otmara',\n",
       " 'grint',\n",
       " 'dougray',\n",
       " 'pasqualino',\n",
       " 'laviscount',\n",
       " 'jitka',\n",
       " 'maleckova',\n",
       " 'papert',\n",
       " 'realpage',\n",
       " 'cnnmembers',\n",
       " 'deepereven',\n",
       " 'phosphorescent',\n",
       " 'tunnelbut',\n",
       " 'evenif',\n",
       " 'andrena',\n",
       " 'highbush',\n",
       " 'pollinating',\n",
       " 'claytonia',\n",
       " 'virginica',\n",
       " 'gamelan',\n",
       " 'pulcinella',\n",
       " 'galliard',\n",
       " 'jaroso',\n",
       " 'benteau',\n",
       " 'wilmers',\n",
       " 'needbank',\n",
       " 'choosefedex',\n",
       " 'servicewhen',\n",
       " 'daysaft',\n",
       " 'recordon',\n",
       " 'coogs',\n",
       " 'stormouflage',\n",
       " 'guarura',\n",
       " 'cacho',\n",
       " 'fap',\n",
       " 'earchopper',\n",
       " 'denzell',\n",
       " 'compli',\n",
       " 'praline',\n",
       " 'saugus',\n",
       " 'obejas',\n",
       " 'fooling',\n",
       " 'hewlitt',\n",
       " 'hogballs',\n",
       " 'raidership',\n",
       " 'unround',\n",
       " 'kalameh',\n",
       " 'ashbel',\n",
       " 'cmsp',\n",
       " 'andonis',\n",
       " 'sakellaris',\n",
       " 'ephthimolous',\n",
       " 'dimaris',\n",
       " 'autocredit',\n",
       " 'pawnbroker',\n",
       " 'clientist',\n",
       " 'vangelis',\n",
       " 'sterno',\n",
       " 'secessionism',\n",
       " 'nerelli',\n",
       " 'nonsignificant',\n",
       " 'witters',\n",
       " 'zammitti',\n",
       " 'mehrene',\n",
       " 'deceived',\n",
       " 'abider',\n",
       " 'apatheid',\n",
       " 'enterprisewide',\n",
       " 'forsthoefel',\n",
       " 'yahtzee',\n",
       " 'leicy',\n",
       " 'toldjaso',\n",
       " 'aond',\n",
       " 'swabian',\n",
       " 'nonidentity',\n",
       " 'falschkauf',\n",
       " 'repuplicans',\n",
       " 'privitzed',\n",
       " 'primitiv',\n",
       " 'degreed',\n",
       " 'televangilism',\n",
       " 'infilitration',\n",
       " 'beevis',\n",
       " 'kenners',\n",
       " 'rwandian',\n",
       " 'devivifi',\n",
       " 'fallujuh',\n",
       " 'colonializ',\n",
       " 'federici',\n",
       " 'toback',\n",
       " 'superbodie',\n",
       " 'cloughly',\n",
       " 'kpax',\n",
       " 'apolinario',\n",
       " 'alete',\n",
       " 'myfordtouch',\n",
       " 'magnino',\n",
       " 'expectaton',\n",
       " 'intralot',\n",
       " 'techn',\n",
       " 'casomorphin',\n",
       " 'vanlandingham',\n",
       " 'fernie',\n",
       " 'huffed',\n",
       " 'hastwo',\n",
       " 'amerli',\n",
       " 'qayarah',\n",
       " 'sabak',\n",
       " 'yezen',\n",
       " 'meshaan',\n",
       " 'takfirism',\n",
       " 'cruc',\n",
       " 'ntox',\n",
       " 'esencia',\n",
       " 'dragonhorn',\n",
       " 'woohabs',\n",
       " 'seahorn',\n",
       " 'picosso',\n",
       " 'sneakerz',\n",
       " 'edric',\n",
       " 'dukecity',\n",
       " 'swampcoolers',\n",
       " 'haskard',\n",
       " 'pintauro',\n",
       " 'mushnik',\n",
       " 'ahrend',\n",
       " 'deuble',\n",
       " 'alonerly',\n",
       " 'peepshow',\n",
       " 'foxxx',\n",
       " 'iyah',\n",
       " 'concertowith',\n",
       " 'dumbarton',\n",
       " 'jackiw',\n",
       " 'tikhovidova',\n",
       " 'saens',\n",
       " 'piazolla',\n",
       " 'exsultate',\n",
       " 'solti',\n",
       " 'feddeck',\n",
       " 'lenn',\n",
       " 'barbwires',\n",
       " 'dizzle',\n",
       " 'frequencies',\n",
       " 'daemonium',\n",
       " 'kinabrew',\n",
       " 'lifetree',\n",
       " 'indemnified',\n",
       " 'batz',\n",
       " 'derangers',\n",
       " 'meddles',\n",
       " 'posterboard',\n",
       " 'dailypaul',\n",
       " 'garyjohnson',\n",
       " 'glpi',\n",
       " 'otcqb',\n",
       " 'preholiday',\n",
       " 'indespensable',\n",
       " 'querstion',\n",
       " 'distortedly',\n",
       " ...]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countvocab(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import RidgeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_cols = ['ADJ_count', 'ADV_count', 'PRON_count', 'NOUN_count', 'PROPN_count', \n",
    "                'strongsubj_count', 'weaksubj_count', 'aux_count', 'pass_count',\n",
    "               'sent_length', 'word_length', 'excl_count', 'quest_count', 'extra_punct_count',\n",
    "               'anger_count', 'fear_count', 'sadness_count', 'joy_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small article dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('byarticle_df_spacy.csv', encoding='utf8', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# balance classes in byarticle dataset (238, 242 -> 480)\n",
    "biased = df.loc[df.label == True]\n",
    "unbiased = df.loc[df.label == False]\n",
    "df = pd.concat([biased, unbiased.sample(n=242).reset_index(drop=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_df = pd.DataFrame()\n",
    "feature_df['label'] = df.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_df['ADJ_count'] = df['text_pos'].apply(count_adj)\n",
    "feature_df['ADV_count'] = df['text_pos'].apply(count_adv)\n",
    "feature_df['PRON_count'] = df['text_pos'].apply(count_pron)\n",
    "feature_df['NOUN_count'] = df['text_pos'].apply(count_noun)\n",
    "feature_df['PROPN_count'] = df['text_pos'].apply(count_propn)\n",
    "\n",
    "feature_df['strongsubj_count'] = df['text_lemmas'].apply(count_strongsubj)\n",
    "feature_df['weaksubj_count'] = df['text_lemmas'].apply(count_weaksubj)\n",
    "\n",
    "feature_df['aux_count'] = df['text_deps'].apply(count_aux)\n",
    "feature_df['pass_count'] = df['text_deps'].apply(count_pass)\n",
    "\n",
    "feature_df['sent_length'] = df['text'].apply(avg_sent_length)\n",
    "feature_df['word_length'] = df['text'].apply(avg_word_length)\n",
    "feature_df['excl_count'] = df['text'].apply(exclamation_marks)\n",
    "feature_df['quest_count'] = df['text'].apply(question_marks)\n",
    "feature_df['extra_punct_count'] = df['text'].apply(multiple_punct)\n",
    "\n",
    "feature_df['anger_count'] = df['text_lemmas'].apply(count_anger)\n",
    "feature_df['fear_count'] = df['text_lemmas'].apply(count_fear)\n",
    "feature_df['sadness_count'] = df['text_lemmas'].apply(count_sadness)\n",
    "feature_df['joy_count'] = df['text_lemmas'].apply(count_joy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>ADJ_count</th>\n",
       "      <th>ADV_count</th>\n",
       "      <th>PRON_count</th>\n",
       "      <th>NOUN_count</th>\n",
       "      <th>PROPN_count</th>\n",
       "      <th>strongsubj_count</th>\n",
       "      <th>weaksubj_count</th>\n",
       "      <th>aux_count</th>\n",
       "      <th>pass_count</th>\n",
       "      <th>sent_length</th>\n",
       "      <th>word_length</th>\n",
       "      <th>excl_count</th>\n",
       "      <th>quest_count</th>\n",
       "      <th>extra_punct_count</th>\n",
       "      <th>anger_count</th>\n",
       "      <th>fear_count</th>\n",
       "      <th>sadness_count</th>\n",
       "      <th>joy_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>0.081754</td>\n",
       "      <td>0.040877</td>\n",
       "      <td>0.021327</td>\n",
       "      <td>0.199052</td>\n",
       "      <td>0.056280</td>\n",
       "      <td>0.085561</td>\n",
       "      <td>0.187166</td>\n",
       "      <td>0.057464</td>\n",
       "      <td>0.005924</td>\n",
       "      <td>27.442623</td>\n",
       "      <td>5.193548</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021390</td>\n",
       "      <td>0.048128</td>\n",
       "      <td>0.034759</td>\n",
       "      <td>0.053476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>0.104247</td>\n",
       "      <td>0.088803</td>\n",
       "      <td>0.015444</td>\n",
       "      <td>0.185328</td>\n",
       "      <td>0.084942</td>\n",
       "      <td>0.129496</td>\n",
       "      <td>0.172662</td>\n",
       "      <td>0.046332</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>5.299242</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050360</td>\n",
       "      <td>0.079137</td>\n",
       "      <td>0.071942</td>\n",
       "      <td>0.050360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>0.063492</td>\n",
       "      <td>0.063492</td>\n",
       "      <td>0.021164</td>\n",
       "      <td>0.201058</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.116505</td>\n",
       "      <td>0.116505</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.010582</td>\n",
       "      <td>17.727273</td>\n",
       "      <td>5.041026</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038835</td>\n",
       "      <td>0.058252</td>\n",
       "      <td>0.058252</td>\n",
       "      <td>0.038835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>0.110145</td>\n",
       "      <td>0.060870</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.194203</td>\n",
       "      <td>0.063768</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.197531</td>\n",
       "      <td>0.034783</td>\n",
       "      <td>0.005797</td>\n",
       "      <td>14.666667</td>\n",
       "      <td>4.965909</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.049383</td>\n",
       "      <td>0.043210</td>\n",
       "      <td>0.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>True</td>\n",
       "      <td>0.101499</td>\n",
       "      <td>0.075993</td>\n",
       "      <td>0.036813</td>\n",
       "      <td>0.197213</td>\n",
       "      <td>0.023928</td>\n",
       "      <td>0.113235</td>\n",
       "      <td>0.216176</td>\n",
       "      <td>0.043913</td>\n",
       "      <td>0.008151</td>\n",
       "      <td>30.520325</td>\n",
       "      <td>4.937400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013235</td>\n",
       "      <td>0.017647</td>\n",
       "      <td>0.019118</td>\n",
       "      <td>0.045588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  ADJ_count  ADV_count  PRON_count  NOUN_count  PROPN_count  \\\n",
       "0   True   0.081754   0.040877    0.021327    0.199052     0.056280   \n",
       "1   True   0.104247   0.088803    0.015444    0.185328     0.084942   \n",
       "2   True   0.063492   0.063492    0.021164    0.201058     0.095238   \n",
       "3   True   0.110145   0.060870    0.014493    0.194203     0.063768   \n",
       "5   True   0.101499   0.075993    0.036813    0.197213     0.023928   \n",
       "\n",
       "   strongsubj_count  weaksubj_count  aux_count  pass_count  sent_length  \\\n",
       "0          0.085561        0.187166   0.057464    0.005924    27.442623   \n",
       "1          0.129496        0.172662   0.046332    0.000000    24.000000   \n",
       "2          0.116505        0.116505   0.047619    0.010582    17.727273   \n",
       "3          0.185185        0.197531   0.034783    0.005797    14.666667   \n",
       "5          0.113235        0.216176   0.043913    0.008151    30.520325   \n",
       "\n",
       "   word_length  excl_count  quest_count  extra_punct_count  anger_count  \\\n",
       "0     5.193548    0.000000     0.000115                0.0     0.021390   \n",
       "1     5.299242    0.000000     0.000000                0.0     0.050360   \n",
       "2     5.041026    0.000000     0.003052                0.0     0.038835   \n",
       "3     4.965909    0.000572     0.000572                0.0     0.055556   \n",
       "5     4.937400    0.000000     0.000270                0.0     0.013235   \n",
       "\n",
       "   fear_count  sadness_count  joy_count  \n",
       "0    0.048128       0.034759   0.053476  \n",
       "1    0.079137       0.071942   0.050360  \n",
       "2    0.058252       0.058252   0.038835  \n",
       "3    0.049383       0.043210   0.055556  \n",
       "5    0.017647       0.019118   0.045588  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480, 18)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you want all rows, and the feature_cols' columns\n",
    "X = feature_df.loc[:, feature_cols]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480,)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = feature_df.label\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_small = RidgeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 72.92%\n"
     ]
    }
   ],
   "source": [
    "lr_small.fit(X, y)\n",
    "y_pred = lr_small.predict(X)\n",
    "acc = sk.metrics.accuracy_score(y, y_pred)\n",
    "print(\"accuracy: {0:.2f}%\".format(acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 61.57%\n"
     ]
    }
   ],
   "source": [
    "y_pred = lr_small.predict(X_train)\n",
    "acc = sk.metrics.accuracy_score(y_train, y_pred)\n",
    "print(\"accuracy: {0:.2f}%\".format(acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 53.67%\n"
     ]
    }
   ],
   "source": [
    "y_pred = lr_small.predict(X_test)\n",
    "acc = sk.metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy: {0:.2f}%\".format(acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ADJ_count', 1.5973991360031634),\n",
       " ('ADV_count', 1.3348986491919477),\n",
       " ('PRON_count', 0.6973130886604662),\n",
       " ('NOUN_count', -1.23016321062661),\n",
       " ('PROPN_count', -1.1620468124807801),\n",
       " ('strongsubj_count', 2.978597137992384),\n",
       " ('weaksubj_count', 2.645595357455885),\n",
       " ('aux_count', -0.09251754547012407),\n",
       " ('pass_count', -0.27660180128839884),\n",
       " ('sent_length', -0.007577823158137686),\n",
       " ('word_length', 0.1117325295379342),\n",
       " ('excl_count', 0.00815072501107081),\n",
       " ('quest_count', 0.04067398305819997),\n",
       " ('extra_punct_count', -0.003773850110652416),\n",
       " ('anger_count', 0.6622751920714581),\n",
       " ('fear_count', 0.5013505249165046),\n",
       " ('sadness_count', 0.2743968475857176),\n",
       " ('joy_count', 0.2566404470378008)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping = list(zip(feature_cols, lr_small.coef_[0]))\n",
    "mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_files = glob('data/train_df_spacy*.csv')\n",
    "\n",
    "test_df = pd.read_csv('data/test_df_spacy.csv').sample(frac=.01, random_state=123)\n",
    "\n",
    "frames = []\n",
    "for file in train_files:\n",
    "    frames.append(pd.read_csv(file).sample(frac=.01, random_state=123))\n",
    "train_df = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_feature_df = pd.DataFrame()\n",
    "train_feature_df['label'] = train_df.label\n",
    "test_feature_df = pd.DataFrame()\n",
    "test_feature_df['label'] = test_df.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_feature_df['ADJ_count'] = train_df['text_pos'].apply(count_adj)\n",
    "train_feature_df['ADV_count'] = train_df['text_pos'].apply(count_adv)\n",
    "train_feature_df['PRON_count'] = train_df['text_pos'].apply(count_pron)\n",
    "train_feature_df['NOUN_count'] = train_df['text_pos'].apply(count_noun)\n",
    "train_feature_df['PROPN_count'] = train_df['text_pos'].apply(count_propn)\n",
    "\n",
    "train_feature_df['strongsubj_count'] = train_df['text_lemmas'].apply(count_strongsubj)\n",
    "train_feature_df['weaksubj_count'] = train_df['text_lemmas'].apply(count_weaksubj)\n",
    "\n",
    "train_feature_df['aux_count'] = train_df['text_deps'].apply(count_aux)\n",
    "train_feature_df['pass_count'] = train_df['text_deps'].apply(count_pass)\n",
    "\n",
    "train_feature_df['sent_length'] = train_df['text'].apply(avg_sent_length)\n",
    "train_feature_df['word_length'] = train_df['text'].apply(avg_word_length)\n",
    "train_feature_df['excl_count'] = train_df['text'].apply(exclamation_marks)\n",
    "train_feature_df['quest_count'] = train_df['text'].apply(question_marks)\n",
    "train_feature_df['extra_punct_count'] = train_df['text'].apply(multiple_punct)\n",
    "\n",
    "train_feature_df['anger_count'] = train_df['text_lemmas'].apply(count_anger)\n",
    "train_feature_df['fear_count'] = train_df['text_lemmas'].apply(count_fear)\n",
    "train_feature_df['sadness_count'] = train_df['text_lemmas'].apply(count_sadness)\n",
    "train_feature_df['joy_count'] = train_df['text_lemmas'].apply(count_joy)\n",
    "\n",
    "test_feature_df['ADJ_count'] = test_df['text_pos'].apply(count_adj)\n",
    "test_feature_df['ADV_count'] = test_df['text_pos'].apply(count_adv)\n",
    "test_feature_df['PRON_count'] = test_df['text_pos'].apply(count_pron)\n",
    "test_feature_df['NOUN_count'] = test_df['text_pos'].apply(count_noun)\n",
    "test_feature_df['PROPN_count'] = test_df['text_pos'].apply(count_propn)\n",
    "\n",
    "test_feature_df['strongsubj_count'] = test_df['text_lemmas'].apply(count_strongsubj)\n",
    "test_feature_df['weaksubj_count'] = test_df['text_lemmas'].apply(count_weaksubj)\n",
    "\n",
    "test_feature_df['aux_count'] = test_df['text_deps'].apply(count_aux)\n",
    "test_feature_df['pass_count'] = test_df['text_deps'].apply(count_pass)\n",
    "\n",
    "test_feature_df['sent_length'] = test_df['text'].apply(avg_sent_length)\n",
    "test_feature_df['word_length'] = test_df['text'].apply(avg_word_length)\n",
    "test_feature_df['excl_count'] = test_df['text'].apply(exclamation_marks)\n",
    "test_feature_df['quest_count'] = test_df['text'].apply(question_marks)\n",
    "test_feature_df['extra_punct_count'] = test_df['text'].apply(multiple_punct)\n",
    "\n",
    "test_feature_df['anger_count'] = test_df['text_lemmas'].apply(count_anger)\n",
    "test_feature_df['fear_count'] = test_df['text_lemmas'].apply(count_fear)\n",
    "test_feature_df['sadness_count'] = test_df['text_lemmas'].apply(count_sadness)\n",
    "test_feature_df['joy_count'] = test_df['text_lemmas'].apply(count_joy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>ADJ_count</th>\n",
       "      <th>ADV_count</th>\n",
       "      <th>PRON_count</th>\n",
       "      <th>NOUN_count</th>\n",
       "      <th>PROPN_count</th>\n",
       "      <th>strongsubj_count</th>\n",
       "      <th>weaksubj_count</th>\n",
       "      <th>aux_count</th>\n",
       "      <th>pass_count</th>\n",
       "      <th>sent_length</th>\n",
       "      <th>word_length</th>\n",
       "      <th>excl_count</th>\n",
       "      <th>quest_count</th>\n",
       "      <th>extra_punct_count</th>\n",
       "      <th>anger_count</th>\n",
       "      <th>fear_count</th>\n",
       "      <th>sadness_count</th>\n",
       "      <th>joy_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42083</th>\n",
       "      <td>True</td>\n",
       "      <td>0.079596</td>\n",
       "      <td>0.057175</td>\n",
       "      <td>0.031390</td>\n",
       "      <td>0.202915</td>\n",
       "      <td>0.085202</td>\n",
       "      <td>0.087819</td>\n",
       "      <td>0.172805</td>\n",
       "      <td>0.045964</td>\n",
       "      <td>0.007848</td>\n",
       "      <td>28.419355</td>\n",
       "      <td>5.140749</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016997</td>\n",
       "      <td>0.031161</td>\n",
       "      <td>0.014164</td>\n",
       "      <td>0.042493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71825</th>\n",
       "      <td>True</td>\n",
       "      <td>0.115226</td>\n",
       "      <td>0.062757</td>\n",
       "      <td>0.049383</td>\n",
       "      <td>0.197531</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>0.151261</td>\n",
       "      <td>0.210084</td>\n",
       "      <td>0.061728</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>22.511628</td>\n",
       "      <td>5.076446</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001221</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025210</td>\n",
       "      <td>0.028011</td>\n",
       "      <td>0.030812</td>\n",
       "      <td>0.081232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99535</th>\n",
       "      <td>False</td>\n",
       "      <td>0.048980</td>\n",
       "      <td>0.065306</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.216327</td>\n",
       "      <td>0.093878</td>\n",
       "      <td>0.073529</td>\n",
       "      <td>0.191176</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.004082</td>\n",
       "      <td>27.111111</td>\n",
       "      <td>5.114754</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.051471</td>\n",
       "      <td>0.066176</td>\n",
       "      <td>0.051471</td>\n",
       "      <td>0.036765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47879</th>\n",
       "      <td>True</td>\n",
       "      <td>0.088710</td>\n",
       "      <td>0.024194</td>\n",
       "      <td>0.018433</td>\n",
       "      <td>0.185484</td>\n",
       "      <td>0.130184</td>\n",
       "      <td>0.114613</td>\n",
       "      <td>0.197708</td>\n",
       "      <td>0.048387</td>\n",
       "      <td>0.012673</td>\n",
       "      <td>47.722222</td>\n",
       "      <td>5.484284</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.042980</td>\n",
       "      <td>0.068768</td>\n",
       "      <td>0.025788</td>\n",
       "      <td>0.048711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36734</th>\n",
       "      <td>False</td>\n",
       "      <td>0.082324</td>\n",
       "      <td>0.053269</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.099274</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.152632</td>\n",
       "      <td>0.055690</td>\n",
       "      <td>0.012107</td>\n",
       "      <td>25.687500</td>\n",
       "      <td>4.909976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.036842</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.031579</td>\n",
       "      <td>0.078947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  ADJ_count  ADV_count  PRON_count  NOUN_count  PROPN_count  \\\n",
       "42083   True   0.079596   0.057175    0.031390    0.202915     0.085202   \n",
       "71825   True   0.115226   0.062757    0.049383    0.197531     0.012346   \n",
       "99535  False   0.048980   0.065306    0.028571    0.216327     0.093878   \n",
       "47879   True   0.088710   0.024194    0.018433    0.185484     0.130184   \n",
       "36734  False   0.082324   0.053269    0.050847    0.152542     0.099274   \n",
       "\n",
       "       strongsubj_count  weaksubj_count  aux_count  pass_count  sent_length  \\\n",
       "42083          0.087819        0.172805   0.045964    0.007848    28.419355   \n",
       "71825          0.151261        0.210084   0.061728    0.012346    22.511628   \n",
       "99535          0.073529        0.191176   0.040816    0.004082    27.111111   \n",
       "47879          0.114613        0.197708   0.048387    0.012673    47.722222   \n",
       "36734          0.100000        0.152632   0.055690    0.012107    25.687500   \n",
       "\n",
       "       word_length  excl_count  quest_count  extra_punct_count  anger_count  \\\n",
       "42083     5.140749         0.0     0.000000                0.0     0.016997   \n",
       "71825     5.076446         0.0     0.001221                0.0     0.025210   \n",
       "99535     5.114754         0.0     0.000000                0.0     0.051471   \n",
       "47879     5.484284         0.0     0.000000                0.0     0.042980   \n",
       "36734     4.909976         0.0     0.000000                0.0     0.036842   \n",
       "\n",
       "       fear_count  sadness_count  joy_count  \n",
       "42083    0.031161       0.014164   0.042493  \n",
       "71825    0.028011       0.030812   0.081232  \n",
       "99535    0.066176       0.051471   0.036765  \n",
       "47879    0.068768       0.025788   0.048711  \n",
       "36734    0.052632       0.031579   0.078947  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = train_feature_df.loc[:, feature_cols]\n",
    "X_test = test_feature_df.loc[:, feature_cols]\n",
    "y_train = train_feature_df.label\n",
    "y_test = test_feature_df.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = RidgeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 53.13%\n"
     ]
    }
   ],
   "source": [
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "acc = sk.metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy: {0:.2f}%\".format(acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 66.27%\n"
     ]
    }
   ],
   "source": [
    "y_pred = lr.predict(X_train)\n",
    "acc = sk.metrics.accuracy_score(y_train, y_pred)\n",
    "print(\"accuracy: {0:.2f}%\".format(acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 69.38%\n"
     ]
    }
   ],
   "source": [
    "y_pred = lr.predict(X)\n",
    "acc = sk.metrics.accuracy_score(y, y_pred)\n",
    "print(\"accuracy: {0:.2f}%\".format(acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ADJ_count', 2.037294218853471),\n",
       " ('ADV_count', 4.756361079894811),\n",
       " ('PRON_count', -2.866549812845596),\n",
       " ('NOUN_count', -0.32467116357211917),\n",
       " ('PROPN_count', -1.0361594868163944),\n",
       " ('strongsubj_count', 3.280423701948726),\n",
       " ('weaksubj_count', 2.852551020529775),\n",
       " ('aux_count', -2.1035910075925166),\n",
       " ('pass_count', -2.8342403779351155),\n",
       " ('sent_length', -0.00049036613253624),\n",
       " ('word_length', -0.00886369926566209),\n",
       " ('excl_count', 0.2644870115728918),\n",
       " ('quest_count', 0.4030525881400419),\n",
       " ('extra_punct_count', 0.04551642843694889),\n",
       " ('anger_count', 0.06463109944606628),\n",
       " ('fear_count', -2.09121604597752),\n",
       " ('sadness_count', -0.9375010537154642),\n",
       " ('joy_count', -4.387512056020496)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping = list(zip(feature_cols, lr.coef_[0]))\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_total = pd.concat([X_train.sample(frac=.25, random_state=123), X_test])\n",
    "y_total = [0]*len(X_test) + [1]*len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = cross_val_score(lr, X_total, y_total, cv=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.655, 0.605, 0.63 , 0.67 , 0.655, 0.62 , 0.6  , 0.63 , 0.63 ,\n",
       "       0.585, 0.66 , 0.655, 0.605, 0.64 , 0.61 ])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(scores)/len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 53.57%\n"
     ]
    }
   ],
   "source": [
    "lr.fit(X_test, y_test)\n",
    "y_pred = lr.predict(X_train)\n",
    "acc = sk.metrics.accuracy_score(y_train, y_pred)\n",
    "print(\"accuracy: {0:.2f}%\".format(acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 58.33%\n"
     ]
    }
   ],
   "source": [
    "y_pred = lr.predict(X_test)\n",
    "acc = sk.metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy: {0:.2f}%\".format(acc*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
