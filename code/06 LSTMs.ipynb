{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib import rnn\n",
    "from random import randint\n",
    "import gensim\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows the development of an LSTM model using TensorFlow. Data is read from preprocessed texts in a CSV, transformed into the appropriate form for the model, then fed into an LSTM.\n",
    "\n",
    "Note that due to computational constraints, this was entirely done on the University of Potsdam GPU servers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hyperpartisan-news-detection\n"
     ]
    }
   ],
   "source": [
    "%cd '/data/hyperpartisan-news-detection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('df/test_df.csv')['text']\n",
    "y_test = pd.read_csv('df/test_df.csv')['label']\n",
    "X_train = pd.read_csv('df/train_df.csv')['text']\n",
    "y_train = pd.read_csv('df/train_df.csv')['label']\n",
    "X_byarticle = pd.read_csv('df/byarticle_df.csv')['text']\n",
    "y_byarticle = pd.read_csv('df/byarticle_df.csv')['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('df/test_agree_df.csv')['text']\n",
    "y_test = pd.read_csv('df/test_agree_df.csv')['label']\n",
    "X_train = pd.read_csv('df/train_agree_df.csv')['text']\n",
    "y_train = pd.read_csv('df/train_agree_df.csv')['label']\n",
    "X_byarticle = pd.read_csv('df/byarticle_df.csv')['text']\n",
    "y_byarticle = pd.read_csv('df/byarticle_df.csv')['label']\n",
    "#X_byarticle = pd.read_csv('df/byarticle_agree_df.csv')['text']\n",
    "#y_byarticle = pd.read_csv('df/byarticle_agree_df.csv')['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finish data input"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train_main, X_train_holdout, y_train_main, y_train_holdout = train_test_split(X_train, y_train, test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_test = np.array(enc.fit_transform(np.array(y_test).reshape(-1, 1)))\n",
    "y_train = np.array(enc.fit_transform(np.array(y_train).reshape(-1, 1)))\n",
    "#y_train_main = np.array(enc.fit_transform(np.array(y_train_main).reshape(-1, 1)))\n",
    "#y_train_holdout = np.array(enc.fit_transform(np.array(y_train_holdout).reshape(-1, 1)))\n",
    "y_byarticle = np.array(enc.fit_transform(np.array(y_byarticle).reshape(-1, 1)))\n",
    "#y_clean = np.array(enc.fit_transform(np.array(y_clean).reshape(-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_total = np.array(X_total)\n",
    "#X_clean = np.array(X_clean)\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "#X_train_main = np.array(X_train_main)\n",
    "#X_train_holdout = np.array(X_train_holdout)\n",
    "X_byarticle = np.array(X_byarticle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_total = np.concatenate((X_train, X_test, X_byarticle))\n",
    "y_total = np.concatenate((y_train, y_test, y_byarticle))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare word embedding matrices (one-time necessity)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "words = []\n",
    "idx = 0\n",
    "word2idx = {}\n",
    "vectors = np.zeros((400000,100))\n",
    "\n",
    "#with open('word-embeddings/glove.6B.50d.txt') as f:\n",
    "with open('word-embeddings/glove.6B.100d.txt') as f:\n",
    "    for l in f:\n",
    "        line = l.split()\n",
    "        word = line[0]\n",
    "        words.append(word)\n",
    "        word2idx[word] = idx\n",
    "        vect = np.array(line[1:]).astype(np.float)\n",
    "        vectors[idx] = vect\n",
    "        idx += 1\n",
    "\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# add zero vector for padding\n",
    "#vectors = np.append(vectors, np.zeros([1, 50]), axis=0)\n",
    "vectors = np.append(vectors, np.zeros([1, 100]), axis=0)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#pickle.dump(word2idx, open(\"word-embeddings/glove_indices.pkl\", \"wb\"))\n",
    "pickle.dump(word2idx, open(\"word-embeddings/glove100_indices.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#np.save('word-embeddings/glove_vectors', vectors)\n",
    "np.save('word-embeddings/glove100_vectors', vectors)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('word-embeddings/GoogleNews-vectors-negative300.bin', binary=True) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "word2index = {token: token_index for token_index, token in enumerate(model.index2word)} "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pickle.dump(word2index, open(\"word-embeddings/word2vec_indices.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "embedding_matrix = np.zeros((len(model.vocab), 300))\n",
    "for i in range(len(model.vocab)):\n",
    "    embedding_vector = model[model.index2word[i]]\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.save('word-embeddings/embedding_matrix', embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "lstm_units = 50\n",
    "num_classes = 2\n",
    "max_sent_length = 100\n",
    "iterations = 16000 #8k for train-main, 12k for total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(457197, 50)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#embedding_matrix = np.load('word-embeddings/glove100_vectors.npy')\n",
    "#word2idx = pickle.load(open(\"word-embeddings/glove100_indices.pkl\", \"rb\" ))\n",
    "embedding_matrix = np.load('word-embeddings/embed_total_vec.npy')\n",
    "word2idx = pickle.load(open(\"word-embeddings/embed_total_idx.pkl\", \"rb\" ))\n",
    "\n",
    "word_embed_size = embedding_matrix.shape[1]\n",
    "embed_vocab_size = embedding_matrix.shape[0]\n",
    "\n",
    "embedding_matrix = np.append(embedding_matrix, np.zeros([1, word_embed_size]), axis=0)\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nextBatch(batch_size, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    Takes list of words, transforms to padded, uniform length vectors of word indices in embedding.\n",
    "    '''\n",
    "    \n",
    "    data_shuffle = np.zeros([batch_size, max_sent_length])\n",
    "    labels_shuffle = np.zeros([batch_size, 2])\n",
    "    \n",
    "    idx = np.arange(0, len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:batch_size]\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        if type(data[idx[i]]) == str:\n",
    "            sent = word_tokenize(data[idx[i]])\n",
    "        elif type(data[idx[i]]) == np.str_:\n",
    "            sent = word_tokenize(data[idx[i]])\n",
    "        else:\n",
    "            sent = []\n",
    "            \n",
    "        for word_idx in range(max_sent_length):\n",
    "            try:\n",
    "                word = str(sent[word_idx])\n",
    "                data_shuffle[i][word_idx] = word2idx[word]\n",
    "            except KeyError:\n",
    "                data_shuffle[i][word_idx] = embed_vocab_size\n",
    "            except IndexError:\n",
    "                data_shuffle[i][word_idx] = embed_vocab_size\n",
    "        labels_shuffle[i] = labels[idx[i]]\n",
    "        \n",
    "    return data_shuffle, labels_shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    embedding_weights = tf.Variable(tf.constant(0.0, shape=[embed_vocab_size+1, word_embed_size]),trainable=True, name=\"embedding_weights\") \n",
    "    embedding_placeholder = tf.placeholder(tf.float32, [embed_vocab_size+1, word_embed_size])\n",
    "    embedding_init = embedding_weights.assign(embedding_placeholder)\n",
    "\n",
    "x_input = tf.placeholder(tf.int64, [None, max_sent_length])\n",
    "x = tf.nn.embedding_lookup(embedding_weights, x_input)\n",
    "y = tf.placeholder(tf.float32, [None, num_classes], name=\"y\")\n",
    "\n",
    "lstm_cell = tf.contrib.rnn.LSTMCell(lstm_units, name=\"lstm_cell\")\n",
    "lstm_cell = tf.contrib.rnn.DropoutWrapper(cell=lstm_cell, output_keep_prob=0.8)\n",
    "value, _ = tf.nn.dynamic_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "weights = tf.Variable(tf.truncated_normal([lstm_units, num_classes]), name=\"weights\")\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"bias\")\n",
    "value = tf.transpose(value, [1, 0, 2])\n",
    "last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "prediction = (tf.matmul(last, weights) + bias)\n",
    "pred_val = tf.argmax(prediction,1)\n",
    "\n",
    "correct = tf.equal(pred_val, tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.7258053e+00, -8.9037985e-01, -3.3093975e+00, ...,\n",
       "        -3.6481032e+00,  9.2985529e-01, -3.1523368e+00],\n",
       "       [-4.7811241e+00,  2.8778939e+00, -3.1557155e+00, ...,\n",
       "         2.4104493e+00, -6.6318178e+00, -2.5443816e+00],\n",
       "       [-4.0230713e+00, -6.2868729e-02, -3.0445645e+00, ...,\n",
       "        -1.4144707e+00, -2.5390584e+00, -5.9339542e+00],\n",
       "       ...,\n",
       "       [ 1.1209801e-03,  4.1167468e-02, -9.9538632e-02, ...,\n",
       "         5.9944548e-02,  8.0686755e-02,  1.2080799e-01],\n",
       "       [-4.2032246e-03, -1.6227258e-02,  8.6567029e-03, ...,\n",
       "         4.1860022e-02,  9.4838940e-02,  6.2755957e-02],\n",
       "       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(embedding_init, feed_dict={embedding_placeholder: embedding_matrix})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "0.598\n",
      "0.54\n",
      "0.5472868\n",
      "iteration 500\n",
      "0.854\n",
      "0.812\n",
      "0.5534884\n",
      "iteration 1000\n",
      "0.912\n",
      "0.888\n",
      "0.5674419\n",
      "iteration 1500\n",
      "0.918\n",
      "0.838\n",
      "0.6139535\n",
      "iteration 2000\n",
      "0.946\n",
      "0.93\n",
      "0.5782946\n",
      "iteration 2500\n",
      "0.956\n",
      "0.92\n",
      "0.6139535\n",
      "iteration 3000\n",
      "0.954\n",
      "0.908\n",
      "0.5379845\n",
      "iteration 3500\n",
      "0.97\n",
      "0.94\n",
      "0.5922481\n",
      "iteration 4000\n",
      "0.958\n",
      "0.946\n",
      "0.56434107\n",
      "iteration 4500\n",
      "0.956\n",
      "0.946\n",
      "0.58294576\n",
      "iteration 5000\n",
      "0.962\n",
      "0.944\n",
      "0.56589144\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-bf85c8c4e89b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mbatch_titles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_lb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnextBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_total\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_total\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_titles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_lb\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(iterations):\n",
    "    batch_titles, batch_lb = nextBatch(batch_size, X_total, y_total)\n",
    "    sess.run(optimizer, feed_dict = {x_input: batch_titles, y: batch_lb})\n",
    "    \n",
    "    if i % 500 == 0:\n",
    "        print('iteration ' + str(i))\n",
    "        batch_titles, batch_lb = nextBatch(500, X_train, y_train)\n",
    "        print(sess.run(accuracy, feed_dict={x_input: batch_titles, y: batch_lb}))\n",
    "        batch_titles, batch_lb = nextBatch(500, X_test, y_test)\n",
    "        print(sess.run(accuracy, feed_dict={x_input: batch_titles, y: batch_lb}))\n",
    "        batch_titles, batch_lb = nextBatch(len(X_byarticle), X_byarticle, y_byarticle)\n",
    "        print(sess.run(accuracy, feed_dict={x_input: batch_titles, y: batch_lb}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/saved_model/simple_save.py:85: calling SavedModelBuilder.add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Pass your op to the equivalent parameter main_op instead.\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /project/cramerus/LSTM-final/saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.simple_save(sess,\n",
    "            \"/project/cramerus/LSTM-final\",\n",
    "            inputs={\"Text\": x_input},\n",
    "            outputs={\"Prediction\": pred_val})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set:\n"
     ]
    }
   ],
   "source": [
    "# calculate final accuracy ratings\n",
    "print('Accuracy on training set:') #480,000\n",
    "correct = 0.0\n",
    "i = 0\n",
    "while i < len(X_train):\n",
    "    batch_titles, batch_lb = nextBatch(1000, np.array(X_train)[i:i+1000], y_train[i:i+1000])\n",
    "    acc = sess.run(accuracy, feed_dict={x_input: batch_titles, y: batch_lb})\n",
    "    correct += 1000 * acc\n",
    "    i += 1000\n",
    "print(str(correct / len(X_train)))\n",
    "\n",
    "#print('Accuracy on holdout training set:') #120,000\n",
    "#correct = 0.0\n",
    "#i = 0\n",
    "#while i < len(X_train_holdout):\n",
    "#    batch_titles, batch_lb = nextBatch(1000, np.array(X_train_holdout)[i:i+1000], y_train_holdout[i:i+1000])\n",
    "#    acc = sess.run(accuracy, feed_dict={x_input: batch_titles, y: batch_lb})\n",
    "#    correct += 1000 * acc\n",
    "#    i += 1000\n",
    "#print(str(correct / len(X_train_holdout)))\n",
    "\n",
    "print('Accuracy on test set:') #150,000\n",
    "correct = 0.0\n",
    "i = 0\n",
    "while i < len(X_test):\n",
    "    batch_titles, batch_lb = nextBatch(1000, np.array(X_test)[i:i+1000], y_test[i:i+1000])\n",
    "    acc = sess.run(accuracy, feed_dict={x_input: batch_titles, y: batch_lb})\n",
    "    correct += 1000 * acc\n",
    "    i += 1000\n",
    "print(str(correct / len(X_test)))\n",
    "\n",
    "print('Accuracy on baby set:')\n",
    "batch_titles, batch_lb = nextBatch(len(X_byarticle), np.array(X_byarticle), y_byarticle)\n",
    "print(sess.run(accuracy, feed_dict={x_input: batch_titles, y: batch_lb}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Train:\n",
    "Accuracy on training set:\n",
    "0.8370049995183945\n",
    "Accuracy on test set:\n",
    "0.5911999992529551\n",
    "Accuracy on baby set:\n",
    "0.5472868\n",
    "\n",
    "Accuracy on training set:\n",
    "0.5681733333071073\n",
    "Accuracy on test set:\n",
    "0.8237999975681305\n",
    "Accuracy on baby set:\n",
    "0.40465117"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
